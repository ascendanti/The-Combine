version: '3.8'

services:
  dragonfly-cache:
    image: docker.dragonflydb.io/dragonflydb/dragonfly:latest
    container_name: dragonfly-cache
    ports:
      - "6379:6379"
    command:
      - --cache_mode=true
      - --maxmemory=8gb
      - --proactor_threads=8
      - --dbfilename=dump
      - --dir=/data
    volumes:
      - dragonfly_data:/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3

  localai:
    image: localai/localai:latest
    container_name: localai
    ports:
      - "8080:8080"
    volumes:
      - localai_models:/models
    environment:
      - THREADS=10
      - CONTEXT_SIZE=2048
      - DEBUG=false
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/readyz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  kg-summary-worker:
    build:
      context: ./daemon
      dockerfile: Dockerfile
    container_name: kg-summary-worker
    command: python kg_summary_worker.py --watch --interval 7200
    volumes:
      - ./daemon:/app
      - ~/.claude/memory:/root/.claude/memory
    environment:
      - LOCALAI_URL=http://localai:8080/v1
    depends_on:
      localai:
        condition: service_healthy
    restart: unless-stopped

  synthesis-worker:
    build:
      context: ./daemon
      dockerfile: Dockerfile
    container_name: synthesis-worker
    command: python synthesis_worker.py --watch --interval 24
    volumes:
      - ./daemon:/app
      - ./.claude:/root/.claude
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - LOCALAI_URL=http://localai:8080/v1
    depends_on:
      localai:
        condition: service_healthy
    restart: unless-stopped

  autonomous-ingest:
    build:
      context: ./daemon
      dockerfile: Dockerfile
    container_name: autonomous-ingest
    command: python autonomous_ingest.py --watch --interval 300
    volumes:
      - ./daemon:/app
      - ~/.claude/memory:/root/.claude/memory
      - /c/Users/New Employee/Documents/GateofTruth:/watch
      - /c/Users/New Employee/Documents/Obsidian/ClaudeKnowledge:/obsidian
    environment:
      - LOCALAI_URL=http://localai:8080/v1
      - DRAGONFLY_URL=redis://dragonfly-cache:6379
      - BOOK_WATCH_FOLDER=/watch
      - USE_UTF_SCHEMA=true
      - OBSIDIAN_VAULT=/obsidian
      - TELEGRAM_BOT_TOKEN=${TELEGRAM_BOT_TOKEN}
      - TELEGRAM_CHAT_ID=${TELEGRAM_CHAT_ID}
    depends_on:
      localai:
        condition: service_healthy
      dragonfly-cache:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import sys; sys.exit(0)"]
      interval: 60s
      timeout: 10s
      retries: 3

volumes:
  dragonfly_data:
    driver: local
  localai_models:
    driver: local

# Usage:
#   docker-compose up -d                    # Start all services
#   docker-compose up -d dragonfly-cache    # Start cache only
#   docker-compose up -d localai            # Start LocalAI only
#   docker-compose logs -f kg-summary-worker # Watch worker logs
#   docker-compose down                     # Stop all
#
# Services:
#   - dragonfly-cache: 25x faster than Redis (cache layer)
#   - localai: Local LLM inference (FREE tokens)
#   - kg-summary-worker: Async file summarization
