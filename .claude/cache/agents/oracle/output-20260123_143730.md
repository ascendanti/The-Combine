# Research Report: ML/Optimization Repositories for Token Optimization

Generated: 2026-01-23 14:37:30

## Summary

After researching the three repositories, **none are directly applicable** to your use case. You are an API consumer of Claude, not a model trainer. These repos focus on model training optimization (DPO, distributed training) rather than prompt/context management. However, the research uncovered **more relevant alternatives** that directly address token optimization for API consumers.

## Questions Answered

### Q1: Relevance of sheeki03/Few-Word to Claude Code token optimization
**Answer:** Repository could not be found. The GitHub user "sheeki03" does not appear to have a public repository called "Few-Word" related to prompt optimization.
**Source:** GitHub search returned no results
**Confidence:** High (verified via multiple search queries)

### Q2: Relevance of Vance0124/Token-level-Direct-Preference-Optimization
**Answer:** NOT APPLICABLE. This is a **model training technique** (TDPO) for fine-tuning LLMs with human preference data. It modifies model weights via token-level KL divergence constraints. As an API consumer, you cannot apply this - it requires access to model internals and training infrastructure.
**Source:** [GitHub - Vance0124/Token-level-Direct-Preference-Optimization](https://github.com/Vance0124/Token-level-Direct-Preference-Optimization)
**Confidence:** High

### Q3: Relevance of deepspeedai/DeepSpeed
**Answer:** NOT APPLICABLE for your use case. DeepSpeed optimizes **inference serving** and **distributed training** for organizations hosting their own models. Its techniques (tensor parallelism, CUDA kernel fusion, quantization) require model hosting infrastructure. As a Claude API consumer, these optimizations happen on Anthropic's side, not yours.
**Source:** [GitHub - deepspeedai/DeepSpeed](https://github.com/deepspeedai/DeepSpeed)
**Confidence:** High

## Detailed Findings

### Finding 1: Token-level DPO (Not Applicable)

**Source:** [arXiv:2404.11999](https://arxiv.org/abs/2404.11999)

**What it does:**
- Training-time optimization using Bradley-Terry reward model at token level
- Improves alignment vs standard DPO by per-token KL divergence constraints
- Requires: model weights, training dataset, compute for fine-tuning

**Why it's not applicable:**
- You don't train Claude - Anthropic does
- No access to model weights or training loop
- This optimizes model behavior, not prompt efficiency

### Finding 2: DeepSpeed (Not Applicable)

**Source:** [Microsoft Research - DeepSpeed](https://www.microsoft.com/en-us/research/project/deepspeed/)

**What it does:**
- Distributed training (ZeRO, 3D parallelism)
- Inference optimization (kernel fusion, quantization)
- Memory optimization for large models

**Why it's not applicable:**
- Requires hosting your own model
- Optimizes GPU utilization on YOUR hardware
- Anthropic already uses these techniques server-side

### Finding 3: What IS Applicable for API Consumers

**Source:** [Anthropic Context Editing API](https://platform.claude.com/docs/en/build-with-claude/context-editing)

**Relevant techniques for your use case:**

| Technique | What It Does | Applicability |
|-----------|--------------|---------------|
| **LLMLingua** | Compresses prompts 5-20x before sending to API | HIGH - reduces input tokens |
| **Anthropic Context Compaction** | SDK-level auto-summarization at threshold | HIGH - already built into Claude SDK |
| **Semantic Chunking** | Split docs with overlap for retrieval | HIGH - your book-ingest already does this |
| **RAG** | Retrieve only relevant chunks | HIGH - you have this in book-query |
| **Prompt Caching** | Reuse prefixes across requests | MEDIUM - Anthropic API feature |

## Comparison Matrix

| Repository | Target User | Requires | Applicable? |
|------------|-------------|----------|-------------|
| Few-Word | Unknown | N/A (not found) | NO |
| Token-level DPO | Model trainers | Model weights, training infra | NO |
| DeepSpeed | Model hosters | GPU cluster, model hosting | NO |
| **LLMLingua** | API consumers | Just pip install | YES |
| **Anthropic SDK** | API consumers | Already using | YES |

## Recommendations

### For This Codebase

1. **You already have the right approach** - Your `token-optimizer-mcp` (Brotli compression, caching) and `book-ingest.py` (semantic chunking) are correct techniques for API consumers.

2. **Consider LLMLingua for prompt compression** - Microsoft's LLMLingua can compress prompts 5-20x before sending to Claude API. This directly reduces input tokens.

```bash
pip install llmlingua
```

```python
from llmlingua import PromptCompressor
compressor = PromptCompressor()
compressed = compressor.compress_prompt(long_prompt, rate=0.5)
# Send compressed to Claude API
```

3. **Use Anthropic's built-in compaction** - The Claude SDK has `context-management-2025-06-27` beta for automatic context summarization.

4. **Your MAPE controller could monitor token efficiency** - Track tokens-per-task and use the controller to adjust chunk sizes based on comprehension quality.

### Implementation Notes

- LLMLingua uses a small local model (GPT-2 or LLaMA) to identify removable tokens
- Anthropic's compaction is server-side, transparent to you
- Your dragonfly cache + token-optimizer-mcp already provide good client-side optimization
- The DPO/DeepSpeed repos are red herrings for your use case

## Sources

1. [GitHub - Vance0124/Token-level-Direct-Preference-Optimization](https://github.com/Vance0124/Token-level-Direct-Preference-Optimization) - TDPO reference implementation
2. [GitHub - deepspeedai/DeepSpeed](https://github.com/deepspeedai/DeepSpeed) - Microsoft's distributed training library
3. [Microsoft LLMLingua](https://github.com/microsoft/LLMLingua) - Prompt compression for API consumers
4. [Anthropic Context Editing Docs](https://platform.claude.com/docs/en/build-with-claude/context-editing) - Built-in context management
5. [Anthropic Context Compaction Cookbook](https://platform.claude.com/cookbook/tool-use-automatic-context-compaction) - SDK auto-summarization
6. [Factory.ai - Evaluating Context Compression](https://factory.ai/news/evaluating-compression) - Compression benchmarks

## Open Questions

- Does the "Few-Word" repository exist under a different name or user?
- Should LLMLingua be integrated with your book-query pipeline for query compression?
- Is the Anthropic context-management beta worth enabling for long daemon tasks?
